{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f37022-0e88-4f91-aea2-8d24ffd388f6",
   "metadata": {},
   "source": [
    "# 0. Install dependencies (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7297eb0c-ed25-4a2f-b0d1-bd3efbaedeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pypdf      \n",
    "# ! pip install chromadb\n",
    "#!pip install lark\n",
    "# !pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777576b3-6512-4060-bbd5-23246fd8619d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37739b-9b34-4e22-ba90-abb5ae6adc7b",
   "metadata": {},
   "source": [
    "# 1. Set the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1cecf-04ea-459a-b5e9-972887e51fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"  #replace with your token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c180a3-dc80-4678-969b-33110c57b634",
   "metadata": {},
   "source": [
    "# 2. Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784daff-0257-49a6-bab2-fa67aa111f5a",
   "metadata": {},
   "source": [
    "## 2.1 PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61bd1e-bfb1-4aa7-9607-eb3fce2294f8",
   "metadata": {},
   "source": [
    " In this project we will be dealing with PDFs, however other types of Loaders can also be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c7955-a1b5-4dfd-8f18-c0b9805819cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Module 1 .pdf\")    # path to pdf file\n",
    "pages = loader.load()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b08304-3776-4f66-9151-13f7340a26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)     #number of pages in the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60145735-84bf-4eef-a5a3-83018fe1d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[1]     #the second page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52d962-42e9-4edc-8c88-3e035a8a5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2cf39-2d0d-4790-b096-74bbb9058248",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa1073-f417-4eff-9280-8b07a710e407",
   "metadata": {},
   "source": [
    "# 2. Document Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c0c0c-4d69-4965-87e9-60581d7cc235",
   "metadata": {},
   "source": [
    "We will be discussing two main kinds of Splitters - Recursive Character and Character TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b15a3b-006d-482b-a79b-755c7ae772c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5fafc-b24d-436b-a3b2-ba2333038161",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size =26\n",
    "chunk_overlap = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f1fe8-a1c6-4ad0-bcf4-5ba6ae342284",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7075ed8-058f-4caa-a481-0bfa357362eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fbfe8-d6b1-4dab-8dce-97541858b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288e021-e9ae-471d-9abc-2c2bc0bd99c9",
   "metadata": {},
   "source": [
    "since the chunk size is 26 and the length of the text is also 26, the full text acts as the chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f717a-87df-4ceb-bb3c-2314405977f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e89a6-99d2-4439-9c7f-3f1f829b9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837df605-f368-4340-81e0-42ebf8d6af5f",
   "metadata": {},
   "source": [
    "this time the text size was more so the text is split into two chunks with overlap of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64d342-c27d-400a-bf0c-c2e0e6126b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e6291-83f3-4c03-b7c5-cd6befd30f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b2cea-f90b-45ed-9211-3095f8503641",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0bf15a-fb9d-48b5-a8ae-9698ec962be2",
   "metadata": {},
   "source": [
    "each alphabet is counted as one character and the spaces are omitted, this happens because the default separator for characterSplitter is newline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7331b13-ac64-423a-9029-51de6e443613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator = ' '\n",
    ")\n",
    "c_splitter.split_text(text3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ad44f-2734-46a6-89c3-35113923ec1a",
   "metadata": {},
   "source": [
    "here we define the separator as a space, so now it works like we would expect it to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0934f7-e473-4a13-9a63-930f62746e5d",
   "metadata": {},
   "source": [
    "## 2.1 Recurisve Splitting Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1a79b-52f1-4f42-b322-a65ba2e87279",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter` is recommended for generic text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4894f-5b71-46fa-ac1b-a3396c6e3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c781d-1fe5-43d5-9a56-1e1c7f95c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a03d21-6872-46c7-9fb9-adb8e37f8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8e3bc-06ef-4f4c-bc4d-fab000541006",
   "metadata": {},
   "source": [
    "the text is split recursively based on the priority of these separator till the desired chunk size is obtained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ad0b4-9359-4574-a35d-64299b60f39b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c7203-707f-448d-ac20-8ce6bbc0fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9e52d-71a7-42b2-9c38-ea8e56e5134a",
   "metadata": {},
   "source": [
    "# 3. VectorStores & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce03bb-a804-4b1a-8bc2-481449443601",
   "metadata": {},
   "source": [
    "## 3.1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee505fd-1d8c-4256-be9b-63c0542d4f53",
   "metadata": {},
   "source": [
    "Data Loading & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2603d9-9065-4020-b823-8505c3f92512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "\n",
    "    PyPDFLoader(\"Module 1 .pdf\"),\n",
    "    PyPDFLoader(\"Module 2.pdf\"),\n",
    "    PyPDFLoader(\"Module 3 .pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e5ee2-3d5c-4afc-8b2f-74d0a4795eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)    #docs has all the pages of the all the pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120f63f-8776-4df8-9e80-853aba215b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[100].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7899cf-2136-4322-8fe6-83f7219fa763",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[100].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9edbe8-b8f7-40f4-aff1-a9d5cb0b9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61b84d-42d0-4cf3-a052-987c29cd5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb7850-760f-488d-8b0f-9938169cdeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ccd2e-dce2-4b08-800f-084ae9817a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[94].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc198b1-30c0-4217-bfff-f80a76e9357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[94].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a935e-2325-46e3-816f-4059b02a5042",
   "metadata": {},
   "source": [
    "## 3.2 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79268bdb-56e0-43a4-87ad-1f7b9c4279a8",
   "metadata": {},
   "source": [
    " We will use ChromaDB to store the vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554bf51-f084-49d0-a8ec-f8c98e602fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2bb8d-a16b-4374-89e0-6a6fddcd3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7097d2-03d4-4703-beed-2cc4462ea651",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff161b6b-d878-4761-8753-7ff7e737f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf759fa6-be6b-477b-88c4-9bdd1675d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding1, embedding2) #shows how similar the embeddings of sentence1 are to sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d0515-655a-45e1-9444-b37ada04a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding1, embedding3) #shows how similar the embeddings of sentence1 are to sentence3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110e4a0-9487-4e9f-b970-3ef357a43ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding2, embedding3) #shows how similar the embeddings of sentence2 are to sentence3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315af085-2800-42dd-96c1-a05c4063bd45",
   "metadata": {},
   "source": [
    "## 3.3 Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f10c07-8720-4bbd-999f-c73c8443bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'docs/test/'  #define directory which will store the ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7d96f-427c-4943-9b64-bcee920ba6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd524817-fcb1-4756-a285-79b7463a250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ef1b1-768d-4715-9187-6238d757e587",
   "metadata": {},
   "source": [
    "documents are automatically persisted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82990ec2-d4f9-4fee-9deb-d619afd1c733",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92544126-c91b-4fb2-ad24-6a4597baa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is fitts law\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4255db5-ba61-4f88-a789-92a8be0bd526",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=3)   #will search for top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5e3eb-12c6-4983-8e22-743da0d8f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d552f-5f3b-4768-970d-fb132e60c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33280030-3d2d-40bc-bb8c-d3744315315f",
   "metadata": {},
   "source": [
    "# 4. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acfc4c-606a-4813-a2bd-08ae7433acf8",
   "metadata": {},
   "source": [
    "## 4.1 Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3edf6-7575-4583-9684-4ea07e651e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb56e0-f852-4f1c-b690-90be48452096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f30e0-070c-4fd5-b958-0230f21c2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f0796-8f5b-4ed6-9c2d-72ae1d67a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd06309-3e04-4cbe-94dd-de5f029ba79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46912f49-9677-44b5-b364-f745b89a1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6c4c3-2fc9-4baf-a605-46fb688c27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1d695-1e47-4d3f-aa9f-10f843782aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87e3dc-31fc-4343-ac0d-5a62b5e651ef",
   "metadata": {},
   "source": [
    "# 5. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff82aa4-c28e-451c-b7c4-dc114d35b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a18be3-ded5-4d46-b3b8-522793442608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0f69b-f91d-4776-8675-4efee79f4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346607b0-5870-4404-b72f-eab3d8f985fd",
   "metadata": {},
   "source": [
    "## 5.1 RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35746cd5-396b-469f-bf9f-1d7878c8ac4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the question-answering pipeline with GPU support (device=0)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\", device=0)\n",
    "\n",
    "# Use the updated HuggingFacePipeline from the new package\n",
    "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"stuff\"  # Ensure that chain_type=\"stuff\" is used for document retrieval\n",
    ")\n",
    "\n",
    "context = \"Some relevant context extracted from the vector store or another source\"\n",
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
    "\n",
    "# Now provide both question and context to the HuggingFace QA pipeline\n",
    "result = qa_chain.invoke({\"query\": question, \"context\": context})\n",
    "\n",
    "# Output the result\n",
    "print(result[\"result\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44764e17-7cde-4f2f-b269-543c8d03ab64",
   "metadata": {},
   "source": [
    "## 5.2 Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55260a94-4fb2-4315-8846-18e4a17c377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afdaa1-d5d2-4a31-952c-5f231095b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf6f54-7412-484a-bc68-e500e44464ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is probability a class topic?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369e061-aff0-4abe-bef9-ec82136cfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c77fe-3af0-42a5-befb-16128e303d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae270f-72d9-4f09-b4d9-efcca2a5a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"source_documents\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f82ea3-50e5-453e-98cf-f62e7643b424",
   "metadata": {},
   "source": [
    "## 5.3 Working QA with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3d37f-aadf-4247-b6b0-49e58a3f103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the question you want to ask\n",
    "question = input(\"enter question: \")\n",
    "\n",
    "#Perform similarity search on the vector DB to get the most relevant context\n",
    "results = vectordb.similarity_search(query=question, k=2)  # Retrieve the top 1 result\n",
    "\n",
    "# Extract the relevant context from the search result\n",
    "context = results[0].page_content  \n",
    "doc_id = results[0].metadata\n",
    "\n",
    "# Prepare the input for the QA pipeline\n",
    "input_data = {\n",
    "    \"question\": question,\n",
    "    \"context\": context\n",
    "}\n",
    "\n",
    "# Initialize the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\", device = 0) #device=0 for GPU\n",
    "\n",
    "#Use the pipeline to answer the question based on the retrieved context\n",
    "response = qa_pipeline(input_data)\n",
    "\n",
    "# Step 8: Print the answer\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c16f9-f21c-4ebb-8b68-644823187398",
   "metadata": {},
   "source": [
    "# 6. Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4aeb08-8266-4922-a994-f124efd837f4",
   "metadata": {},
   "source": [
    "## 6.1 Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c897144-deb3-4c46-a2af-37314c9b6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc9569-3eb2-48b1-ad41-47d6f9a1457b",
   "metadata": {},
   "source": [
    "## 6.2 Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6c36e-0b5e-47ec-ab26-1f649f92ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80fd89-5f6c-4b6f-b62c-c73b9d3b5e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd1f48-465f-4e70-bb5f-ccf83286e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9549e4-d876-4284-a5ff-bbc9dce1e11a",
   "metadata": {},
   "source": [
    "# Actual working Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0777b1-d44b-48b3-a531-784fe60c290d",
   "metadata": {},
   "source": [
    "run only the below cells for the chatbot to work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94e5d01-ae21-440c-842d-01bde439cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devanshvikram/tensorflow-test/env/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca810466-75ab-42da-98b2-ebd16e6e08a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"  #replace with your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f0f354-7091-460b-b82d-01302fc2d76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/grv5kb4x3d766z_nlmm4jw980000gn/T/ipykernel_87933/2151997787.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "/Users/devanshvikram/tensorflow-test/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'docs/test/'                  #directory to store the chroma db\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b630c6-f9e5-47c3-98dc-f47dc1073974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is fitts law\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: MT = a + b . log2(A/W + 1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  who gave fitts law\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: psychologist Paul Fitts\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what are the design rules\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: psychologist Paul Fitts\n",
      "\n",
      "Design Rules\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "class ChatbotWithMemory:\n",
    "    def __init__(self, vectordb, qa_pipeline):\n",
    "        self.vectordb = vectordb  # The vector database for similarity search\n",
    "        self.qa_pipeline = qa_pipeline  # The QA pipeline model\n",
    "        self.memory = []  # Memory to store conversation history\n",
    "    \n",
    "    def add_to_memory(self, question, answer):\n",
    "        \"\"\"Store question and answer in memory.\"\"\"\n",
    "        self.memory.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    def get_memory_context(self):\n",
    "        \"\"\"Combine memory into a context string.\"\"\"\n",
    "        context = \"\"\n",
    "        for entry in self.memory:\n",
    "            context += f\"Q: {entry['question']}\\nA: {entry['answer']}\\n\"\n",
    "        return context\n",
    "    \n",
    "    def ask_question(self, question, k=2):\n",
    "        \"\"\"Answer a question with memory and context retrieval.\"\"\"\n",
    "        \n",
    "        # Perform similarity search on the vector DB to get the most relevant context\n",
    "        results = self.vectordb.similarity_search(query=question, k=k)\n",
    "        \n",
    "        # Extract the relevant context from the search result\n",
    "        if results:\n",
    "            retrieved_context = results[0].page_content\n",
    "            doc_id = results[0].metadata\n",
    "        else:\n",
    "            retrieved_context = \"\"\n",
    "            doc_id = None\n",
    "\n",
    "        # Combine memory and retrieved context\n",
    "        memory_context = self.get_memory_context()\n",
    "        full_context = memory_context + \"\\n\" + retrieved_context\n",
    "        \n",
    "        # Prepare input for the QA pipeline\n",
    "        input_data = {\n",
    "            \"question\": question,\n",
    "            \"context\": full_context\n",
    "        }\n",
    "\n",
    "        # Use the QA pipeline to generate an answer\n",
    "        response = self.qa_pipeline(input_data)\n",
    "        answer = response['answer']\n",
    "\n",
    "        # Print the answer\n",
    "        print(f\"Answer: {answer}\")\n",
    "        \n",
    "        # Add this interaction to memory\n",
    "        self.add_to_memory(question, answer)\n",
    "        \n",
    "        return answer\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\", device=0)\n",
    "\n",
    "# Initialize chatbot with memory\n",
    "chatbot = ChatbotWithMemory(vectordb, qa_pipeline)\n",
    "\n",
    "# Chatbot interaction example\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    chatbot.ask_question(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be684ab8-c079-4cdb-92c7-60d018a2da28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
